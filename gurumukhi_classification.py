# -*- coding: utf-8 -*-
"""Gurumukhi Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/129GIjduTs7zUn4rT5r_X6FYdcKpFotXt
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
from matplotlib import pyplot as plt
# %matplotlib inline
import os
import cv2
from keras.layers import Dense, Flatten

#Mount the drive
from google.colab import drive
drive.mount('/content/gdrive')

# Define the path for the train folder
trainset_path = "/content/gdrive/MyDrive/Colab Notebooks/NeuralNetwork"

# Set size for the train images
train_size = (32, 32)

# Define lists for storing training images and titles
img_train = []
titles = []

# Run the loop for 0 to 9 folders under the train folder
for title in range(10):
    train_path = os.path.join(trainset_path,'train',str(title))
    # Run the loop for every image file in the folder
    for img_file in os.listdir(train_path):
        img_file_path = os.path.join(train_path, img_file)

        # Resizing of the image
        if img_file_path.endswith(('.tiff','.bmp')):
            image = cv2.imread(img_file_path, cv2.IMREAD_GRAYSCALE)
            image = cv2.resize(image, train_size)

            img_train.append(image)
            titles.append(title)

# Creating NumPy arrays for the training images and titles lists
img_train = np.array(img_train)
titles = np.array(titles)

# Saving the arrays in the Numpy Format
np.save('x_train.npy', img_train)
np.save('y_train.npy', titles)

# Define the path for the validation folder
valset_path = "/content/gdrive/MyDrive/Colab Notebooks/NeuralNetwork"

# Set size for the validation images
val_size = (32, 32)

# Define lists for storing validation images and titles
img_val = []
titles_val = []

# Run the loop for 0 to 9 folders under the validation folder
for title in range(10):
    val_path = os.path.join(valset_path, 'val', str(title))

    # Run the loop for every image file in the folder
    for img_file in os.listdir(val_path):
        img_file_path = os.path.join(val_path, img_file)
        if img_file_path.endswith(('.tiff','.bmp')):
        # Resizing of the image
            image = cv2.imread(img_file_path, cv2.IMREAD_GRAYSCALE)
            image = cv2.resize(image, val_size)

            img_val.append(image)
            titles_val.append(title)

# Creating NumPy arrays for the training images and titles lists
img_val = np.array(img_val)
titles_val = np.array(titles_val)

# Saving the arrays in the Numpy Format
np.save('x_test.npy', img_val)
np.save('y_test.npy', titles_val)

x_train = np.load('x_train.npy')
y_train = np.load('y_train.npy')
x_test = np.load('x_test.npy')
y_test = np.load('y_test.npy')

# Check the shape of the images of training and test data

x_train[0].shape
x_train[0]
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
plt.matshow(x_train[755])
plt.matshow(x_test[100])

#Neural network

nn = keras.Sequential([
keras.layers.Flatten(),keras.layers.Dense(10, input_shape=(1024,),activation = 'sigmoid')
])

# Neural network compilation
nn.compile(optimizer='adam',
loss='sparse_categorical_crossentropy',
metrics=['accuracy']
)

#Train Neural network
nn.fit(x_train, y_train,epochs= 12, validation_data=(x_test, y_test))

#Scaling the dataset and again training the model
x_train_scaled = x_train/255
x_test_scaled = x_test/255
nn.fit(x_train_scaled, y_train,epochs= 12, validation_data=(x_test_scaled, y_test))

#Evaluate the scaled test data
nn.evaluate(x_test_scaled,y_test)

# Predicting Images
plt.matshow(x_test[0])
y_pred = nn.predict(x_test_scaled)

y_pred[0]
print('Predicted Value is ',np.argmax(y_pred[0]))
plt.matshow(x_test[90])
print('Predicted Value is ',np.argmax(y_pred[90]))
plt.matshow(x_test[176])
print('Predicted Value is ',np.argmax(y_pred[176]))

#Confusion matrix to evaluate the prediction

y_pred_title=[np.argmax(i) for i in y_pred]
print(y_pred_title, len(y_pred_title))
confusion_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred_title)
confusion_mat

import seaborn as sn
plt.figure(figsize = (12,12))
sn.heatmap(confusion_mat,annot=True)
plt.xlabel('Predicted values')
plt.ylabel('Actual values')

#Adding layers in model for increaing accuracy
nn2 = keras.Sequential([
keras.layers.Flatten(),
keras.layers.Dense(1024,input_shape=(1024,), activation='relu'),
keras.layers.Dense(10, activation='softmax')
])

#Neural network compilation
nn2.compile(optimizer='adam',
loss='sparse_categorical_crossentropy',
metrics=['accuracy']
)

#Train Neural network
history = nn2.fit(x_train_scaled, y_train,epochs= 12, validation_data=(x_test_scaled, y_test))

#Evaluating modified model
nn2.evaluate(x_test_scaled,y_test)

#Verifying the predictions again using confusion matrix

y_pred = nn2.predict(x_test_scaled)
y_pred[0]
y_pred_title=[np.argmax(i) for i in y_pred]
print(y_pred_title, len(y_pred_title))
confusion_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred_title)
confusion_mat

plt.figure(figsize = (12,12))
sn.heatmap(confusion_mat,annot=True)
plt.xlabel('Predicted values')
plt.ylabel('Actual values')

# Model evaluation 

test_loss, test_accuracy = nn.evaluate(x_test, y_test)
print('Test accuracy of the model is:', test_accuracy)

#Plotting the accuracy of the model with training and validation accuracy

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy of the model')
plt.ylabel('Accuracy')
plt.xlabel('Number of Epoch')
plt.legend(['Train data', 'Validation data'], loc='center right')
plt.show()