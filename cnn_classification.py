# -*- coding: utf-8 -*-
"""Cnn_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lr5fnLfPGWMVlv-R5DzogpEXSjvYJD61
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
import pandas as pd
from matplotlib import pyplot as plt
# %matplotlib inline
import os
import cv2
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten,GlobalAveragePooling2D
from PIL import Image
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report

#Define training and test paths
from google.colab import drive
drive.mount('/content/gdrive')
train_path= "/content/gdrive/MyDrive/Colab Notebooks/train_val"
test_path = "/content/gdrive/MyDrive/Colab Notebooks/test"

from google.colab import files
data_to_load = files.upload()
import io
train_title = pd.read_csv(io.BytesIO(data_to_load['train_v.csv']))

image_train = []
titles = []
for image_file in os.listdir(train_path):
    if image_file.endswith('.png'):
        
        #Resizing the image
        image = cv2.imread(os.path.join(train_path, image_file))
        image = cv2.resize(image, (128, 128))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        img_arr = np.array(image)

        image_train.append(img_arr)
        titles.append(image_file)

#String to numerical 
le = LabelEncoder()
titles = le.fit_transform(titles)

# Creating NumPy arrays for the training images and titles lists
image_train = np.array(image_train)
titles = np.array(titles)

# Saving the arrays in the Numpy Format
np.save('x_train.npy', image_train)
np.save('y_train.npy', titles)

x_train = np.load('x_train.npy')
y_train = np.load('y_train.npy')

x_train.shape,  y_train.shape

x_train[:5]
y_train[:5]

image_val = []
titles = []
for image_file in os.listdir(test_path):
    if image_file.endswith('.png'):
        # Resizing the image
        image = cv2.imread(os.path.join(test_path, image_file))
        image = cv2.resize(image, (128, 128))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        img_arr = np.array(image)

        image_val.append(img_arr)
        titles.append(image_file)

#String to numerical 
le = LabelEncoder()
titles = le.fit_transform(titles)

# Creating NumPy arrays for the training images and titles lists
image_val = np.array(image_val)
titles = np.array(titles)

# Saving the arrays in the Numpy Format
np.save('x_test.npy', image_val)
np.save('y_test.npy', titles)

x_test = np.load('x_test.npy')
y_test = np.load('y_test.npy')

x_test.shape

#Loaded images
plt.figure(figsize = (5,2))
plt.imshow(x_train[200])

plt.figure(figsize = (5,2))
plt.imshow(x_train[350])

# Define image classes
img_class = ['line', 'dot_line', 'hbar_categorical', 'vbar_categorical', 'pie']
img_class[0]

# Map the titles
title_map = {'line': 0, 'dot_line': 1, 'hbar_categorical': 2, 'vbar_categorical': 3, 'pie': 4}
y_train = np.array([title_map[label] for label in train_title['type']])

y_train.shape, y_test.shape

def img_sample(x, y, index):
    plt.figure(figsize = (10,2))
    plt.imshow(x[index])
    plt.xlabel(img_class[y[index]])

img_sample(x_train,y_train,0)
img_sample(x_train,y_train,200)
img_sample(x_train,y_train,350)

#Image Normalization
x_train=x_train /255
x_test=x_train /255

x_test.shape

y_train_index = train_title['image_index']
y_train_type = train_title['type']

y_train_type[:5]

#Creating Neural Network
nn = Sequential([
    Flatten(input_shape=(128,128,3)),Dense(3000, activation='relu'),
    Dense(1000, activation='relu'),
    Dense(5, activation='softmax')
])
# Neural Network compilation
nn.compile(optimizer='SGD', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
nn.fit(x_train,y_train,epochs=10)

# Train Test split
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

nn.evaluate(x_test,y_test)

y_pred = nn.predict(x_test)
y_pred
y_pred_classes = [np.argmax(ele) for ele in y_pred]

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

#Creating CNN
cnn = Sequential([
    Conv2D(filters=16 ,kernel_size=(3,3), activation='relu', input_shape=(128,128,3)),
    MaxPooling2D(pool_size=(2,2)),
    Conv2D(32, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(5, activation='softmax')
])

# CNN Compilation
cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

#Train CNN
history = cnn.fit(x_train, y_train, batch_size=1000, epochs=50,validation_data=(x_test, y_test))

# Plotting the loss of model
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss of the model')
plt.ylabel('Loss')
plt.xlabel('Number of Epoch')
plt.legend(['Train data', 'Validation data'], loc='center right')
plt.show()

cnn.evaluate(x_test,y_test)

img_sample(x_test,y_test,1)
img_sample(x_test,y_test,50)
img_sample(x_test,y_test,25)
img_sample(x_test,y_test,30)

y_pred = cnn.predict(x_test)
y_pred[:5]

y_classes = [np.argmax(element) for element in y_pred]
y_classes[:5]

y_test[:5]

img_sample(x_test,y_test,15) #actual
img_class[y_classes[15]] #predicted

print(classification_report(y_test,y_classes))

confusion_mat = confusion_matrix(y_test, y_classes)
print('Confusion Matrix:')
print(confusion_mat)

#Confusion matrix
import seaborn as sn
plt.figure(figsize = (10,10))
sn.heatmap(confusion_mat,annot=True)
plt.xlabel('Predicted values')
plt.ylabel('Actual values')

"""### Finetune the Model"""

from keras.applications import VGG16
from keras.layers import Dense, Dropout, Flatten
from keras.models import Model
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator

#Input size of image
image_width, image_height = 224, 224

from google.colab import drive
drive.mount('/content/gdrive')
train_path = "/content/gdrive/MyDrive/Colab Notebooks/train_val/"

val_path = "/content/gdrive/MyDrive/Colab Notebooks/train_val/"

# VGG16 Model
vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(image_width, image_height, 3))

# Layer freeze of pre-trained model
for layer in vgg_model.layers:
    layer.trainable = False

# Adding layers
x = vgg_model.output
x = Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(1, activation='sigmoid')(x)

# A new model with base as pre-trained model and with addition of layers
new_model = Model(inputs=vgg_model.input, outputs=predictions)

# Model compilation with binary cross entropy
new_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-5), metrics=['accuracy'])

# Data Augumentation
train_gen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

val_gen = ImageDataGenerator(rescale=1./255)

batch_size = 16

nb_train_samples = 1500
nb_val_samples = 300

epochs = 12

# Model training with data generators
history = new_model.fit(
    train_gen.flow_from_directory(train_path, target_size=(image_width, image_height), batch_size=batch_size, class_mode='binary'),
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=val_gen.flow_from_directory(val_path, target_size=(image_width, image_height), batch_size=batch_size, class_mode='binary'),
    validation_steps=nb_val_samples // batch_size)

# Model evaluation
test_gen = ImageDataGenerator(rescale=1./255)
from google.colab import drive
drive.mount('/content/gdrive')
test_path = "/content/gdrive/MyDrive/Colab Notebooks/test"

test_generator = test_gen.flow_from_directory(
    test_path,
    target_size=(image_width, image_height),
    batch_size=batch_size,
    class_mode='binary')

test_loss, test_accuracy = new_model.evaluate(test_generator, steps=len(test_generator))

print('Test accuracy is:', test_accuracy)
print('Test loss is:', test_loss)